### ðŸ§± Feature ID 6: Prompt Compiler & OpenAI Integration (Python Microservice)

---

### ðŸŽ¯ Purpose
Create a dedicated Python microservice responsible for:
- Compiling scanner prompts dynamically from config
- Calling OpenAI's ChatCompletion endpoint
- Validating, structuring, and returning GPT-based scan results
- Logging raw prompt and response for dev mode

---

### âœ… Prompt 6.1 â€” Service Setup

Folder Structure:
```
/openai-service/
  â”œâ”€â”€ main.py             # FastAPI entrypoint
  â”œâ”€â”€ prompts/
  â”‚     â””â”€â”€ base_prompt.py
  â”œâ”€â”€ models/
  â”‚     â””â”€â”€ schemas.py    # Response models
  â”œâ”€â”€ config/
  â”‚     â””â”€â”€ scanners/     # *.config.json
  â”œâ”€â”€ utils/
  â”‚     â””â”€â”€ compiler.py   # prompt builder
  â”‚     â””â”€â”€ openai_api.py # API call logic
```

Dependencies:
- FastAPI
- Uvicorn
- httpx
- openai
- pydantic

---

### âœ… Prompt 6.2 â€” API Endpoint: `POST /run`

In `main.py`:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from utils.compiler import compile_prompt
from utils.openai_api import call_openai

class PromptRequest(BaseModel):
    scanner_key: str
    content: str
    context: dict = {}

app = FastAPI()

@app.post("/run")
async def run_prompt(data: PromptRequest):
    try:
        config = load_scanner_config(data.scanner_key)
        payload = compile_prompt(config, data.content, data.context)
        gpt_result = await call_openai(payload)
        return {
            "scanner_key": data.scanner_key,
            "prompt": payload,
            "response": gpt_result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

### âœ… Prompt 6.3 â€” Prompt Compiler

`/utils/compiler.py`:
```python
def compile_prompt(config: dict, content: str, context: dict = {}) -> dict:
    prompt_text = config["prompt"]["user"].replace("{{CONTENT}}", content)
    return {
        "model": config.get("model", "gpt-4"),
        "messages": [
            {"role": "system", "content": config["prompt"]["system"]},
            {"role": "user", "content": prompt_text}
        ],
        "max_tokens": config.get("tokenLimit", 2048)
    }
```

---

### âœ… Prompt 6.4 â€” OpenAI Request Logic

`/utils/openai_api.py`:
```python
import openai

async def call_openai(payload: dict) -> dict:
    openai.api_key = os.getenv("OPENAI_API_KEY")
    response = await openai.ChatCompletion.acreate(**payload)
    return response["choices"][0]["message"]["content"]
```

Features:
- Retry on rate limit
- Capture total tokens used (for telemetry)
- Store raw input/output for dev-mode JSON viewer

---

### âœ… Prompt 6.5 â€” Config Loader (Shared)

`/utils/config.py`:
```python
import json, os

def load_scanner_config(key: str):
    path = os.path.join("config/scanners", f"{key}.config.json")
    with open(path) as f:
        return json.load(f)
```

---

### ðŸ§  Notes
- All outputs returned from this service will be saved in `scan_results` table by Node.js backend
- Logs should include: scanner_key, prompt_length, response_length, tokens_used
- Future: Add streaming support via SSE or WebSocket
